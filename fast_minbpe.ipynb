{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heavily inspired by Karpathy's, here's my late night take on a simple, clean, and fast BPE implementation.\n",
    "\n",
    "I'm borrowing a lot from Karpathy's code, but we'll use more efficient data-structures:\n",
    "1. We'll hold our sequence in an [*IndexedList*](datastructures/indexedlist.py) which is a simple linked list (so allows efficiently deleting elements) and maintains an index from a pair to all of its occurrences in the list (so allows fast iteration).\n",
    "3. We'll calculate the pair counts only once, and maintain them in a [*Multiset*](datastructures/multiset.py). This will allow efficiently finding the next pair to merge.\n",
    "\n",
    "Note that the [*IndexedList*](datastructures/leap.py) maintains a _possibly stale_ index. That means that when iterating on pairs we have to check that each accessed pair indeed still holds the desired value. Note that datastructures that are \"lazy\" like this are often more efficient.\n",
    "\n",
    "The [*Multiset*](datastructures/multiset.py) is functionally equivalent to the built-in `collections.Counter`, but finding the most common element is drastically faster (see [multiset_tests.ipynb](datastructures/multiset_tests.ipynb)).\n",
    "\n",
    "# Why is minbpe slow?\n",
    "\n",
    "If we are to perform N merges, and the length of the training sequence is L, Karpathy's original impl does (I think):\n",
    "```python\n",
    "for i in range(N):\n",
    "    calc_stats()        # O(L)\n",
    "    find_max()          # O(L)\n",
    "    do_merges()         # O(L)\n",
    "```\n",
    "For a total complexity of O(N*L) (maybe I'm neglecting some factors).\n",
    "\n",
    "# Why is fast_minbpe fast?\n",
    "\n",
    "Using our `IndexedList` and `Multiset`, we instead get:\n",
    "```python\n",
    "calc_stats()                      # O(L)\n",
    "for i in range(N):\n",
    "    find_max()                    # O(log(L))\n",
    "    do_merges_and_update_stats()  # O(M_i)\n",
    "```\n",
    "Where M_i denotes the actual number of merges we perform at the ith iteration. Note that M_1+M_2+...+M_n <= L - 1, so the overall complexity of evertyhing (again neglecting logarithmic factors) is O(L)!\n",
    "\n",
    "We'll unfortunately have to give up some lovely code from Karpathy's implementation, such as:\n",
    "```python\n",
    "pair = max(stats, key=stats.get)\n",
    "```\n",
    "That said, armed with the `IndexedList` and the `Multiset`, our code remains concise and clean I think.\n",
    "\n",
    "Note that I only implement the functionality of Karpathy's `BasicTokenizer`.\n",
    "\n",
    "You can find some more details in [this post](https://yanivle.github.io/ai/2024/02/23/fast_minbpe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.mytimeit import timeit\n",
    "from bpe import train, tokenize, detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(filename, vocab_size):\n",
    "    text = open(filename, \"r\", encoding=\"utf-8\").read()\n",
    "    print(f'Source text is of length: {len(text):,}')\n",
    "    print(f'First 100 chars: {repr(text[:100])}')\n",
    "    merge_tree, vocab = timeit(lambda: train(text, vocab_size), 'Training')\n",
    "    tokenized_text = timeit(lambda: tokenize(text, merge_tree), 'Tokenization')\n",
    "    print(f'Tokenized text has {len(tokenized_text)} tokens.')\n",
    "    detokenized_text = timeit(lambda: detokenize(tokenized_text, vocab), 'Detokenize')\n",
    "    assert detokenized_text == text\n",
    "    return tokenized_text, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is of length: 185,561\n",
      "First 100 chars: 'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaTh'\n",
      "Training tokenizer on text of length 185,561 with vocab of size 300.\n",
      "build_indexed_list took 0.21 seconds.\n",
      "init_pairs_stats took 0.01 seconds.\n",
      "Training took 0.36 seconds.\n",
      "Tokenization took 0.33 seconds.\n",
      "Tokenized text has 128451 tokens.\n",
      "Detokenize took 0.01 seconds.\n",
      "\n",
      "Source text is of length: 185,561\n",
      "First 100 chars: 'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaTh'\n",
      "Training tokenizer on text of length 185,561 with vocab of size 1,000.\n",
      "build_indexed_list took 0.32 seconds.\n",
      "init_pairs_stats took 0.02 seconds.\n",
      "Training took 0.82 seconds.\n",
      "Tokenization took 0.38 seconds.\n",
      "Tokenized text has 58337 tokens.\n",
      "Detokenize took 0.00 seconds.\n",
      "\n",
      "Source text is of length: 185,561\n",
      "First 100 chars: 'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaTh'\n",
      "Training tokenizer on text of length 185,561 with vocab of size 10,000.\n",
      "build_indexed_list took 0.15 seconds.\n",
      "init_pairs_stats took 0.02 seconds.\n",
      "Training took 1.00 seconds.\n",
      "Tokenization took 0.52 seconds.\n",
      "Tokenized text has 24306 tokens.\n",
      "Detokenize took 0.00 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vocab_size in [300, 1000, 10_000]:\n",
    "    tokenized_text, vocab = train_and_test(r'data\\taylorswift.txt', vocab_size)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C🍔op🍔y 🍔p🍔ast🍔e 🍔of the 🍔Wikipe🍔dia 🍔article 🍔on 🍔Taylor Swift, 🍔as of 🍔F🍔eb🍔 🍔16🍔, 2024🍔.\n",
      "🍔--🍔-🍔\n",
      "\n",
      "🍔Main 🍔m🍔enu🍔\n",
      "\n",
      "🍔Wikipedia🍔The F🍔ree 🍔Enc🍔yclopedia\n",
      "🍔\n",
      "🍔Search🍔\n",
      "🍔C🍔re🍔ate 🍔account🍔\n",
      "🍔L🍔og🍔 🍔in🍔\n",
      "\n",
      "🍔Personal 🍔tool🍔s\n",
      "🍔Cont🍔ents 🍔 🍔h🍔ide🍔\n",
      "🍔(🍔Top🍔)\n",
      "🍔Life and career🍔\n",
      "Toggle 🍔Life and 🍔career 🍔subsection\n",
      "🍔Artistry🍔\n",
      "Toggle 🍔Artist🍔ry 🍔subsection\n",
      "🍔Accolades and achievements\n",
      "🍔Cultural status🍔\n",
      "Toggle 🍔Cultural 🍔status 🍔subsection\n",
      "🍔Wealth🍔\n",
      "Toggle 🍔Weal🍔th 🍔subsection\n",
      "🍔Discography\n",
      "🍔Filmography\n",
      "🍔Tours\n",
      "🍔See also🍔\n",
      "F🍔ootnotes\n",
      "🍔References\n",
      "🍔Toggle 🍔Referenc🍔es 🍔subsection\n",
      "🍔External links\n",
      "Taylor Swift\n",
      "🍔\n",
      "🍔13🍔6 🍔l🍔ang🍔u🍔ag🍔es\n",
      "🍔Ar🍔tic🍔le\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our tokenized text:\n",
    "def debug(tokenized_text, vocab):\n",
    "    print('🍔'.join([vocab[t].decode('utf-8') for t in tokenized_text]))\n",
    "\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is of length: 185,561\n",
      "First 100 chars: 'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaTh'\n",
      "Training tokenizer on text of length 185,561 with vocab of size 100,000.\n",
      "build_indexed_list took 0.22 seconds.\n",
      "init_pairs_stats took 0.02 seconds.\n",
      "Training took 1.61 seconds.\n",
      "Tokenization took 0.60 seconds.\n",
      "Tokenized text has 1 tokens.\n",
      "Detokenize took 0.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "# What about a GPT-4-like vocabulary with 100K tokens?\n",
    "tokenized_text, vocab = train_and_test(r'data\\taylorswift.txt', 100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.61 seconds, but with a vocabulary of 100K, only 1 token remains... Let's try something longer...\n",
    "\n",
    "[The Guiness book of world records recognizes](https://www.guinnessworldrecords.com/world-records/longest-novel) Marcel Proust's \"A la recherche du temps perdu\" as the world's longest novel. Turns out it's comprised of several volumes. I found a translated version of the first volume - \"Swann's Way\" - on [the website](https://gutenberg.net.au/plusfifty-n-z.html#proust) for Project Gutenberg. Specifically [this file](https://gutenberg.net.au/ebooks03/0300511.txt). It's just over 1MB - perfect!\n",
    "Let's try training a GPT-4 sized tokenizer with a vocabulary of 100K tokens on that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is of length: 1,088,320\n",
      "First 100 chars: \"\\nProject Gutenberg Australia\\n\\nTitle:      Swann's Way\\n            (Du côté de chez Swann)\\n          \"\n",
      "Training tokenizer on text of length 1,088,320 with vocab of size 100,000.\n",
      "build_indexed_list took 1.24 seconds.\n",
      "init_pairs_stats took 0.12 seconds.\n",
      "Training took 8.15 seconds.\n",
      "Tokenization took 3.90 seconds.\n",
      "Tokenized text has 86707 tokens.\n",
      "Detokenize took 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "tokenized_text, vocab = train_and_test(r'data\\0300511.txt', 100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🍔Project Gutenberg Australia🍔\n",
      "\n",
      "🍔Title:      Swann's Way\n",
      "            (Du côté de chez Swann)\n",
      "            [Vol. 1 of Remembrance of Things Past—\n",
      "            (À la Recherche du temps perdu)]\n",
      "Author:     Marcel Proust\n",
      "            Translated from the French by C. K. Scott Moncrieff🍔\n",
      "🍔* 🍔A Project Gutenberg of Australia 🍔eBook 🍔*🍔\n",
      "🍔eBook 🍔No🍔.🍔: 🍔 🍔03🍔0🍔0🍔5🍔1🍔1🍔.🍔t🍔x🍔t\n",
      "🍔L🍔angu🍔age🍔: 🍔  🍔English🍔\n",
      "Date 🍔first 🍔posted🍔: 🍔        🍔 🍔Mar🍔ch 🍔20🍔03🍔\n",
      "Date 🍔most 🍔recently 🍔up🍔d🍔ated🍔: 🍔S🍔ept 🍔20🍔2🍔2🍔\n",
      "\n",
      "🍔P🍔rodu🍔ction 🍔not🍔es: 🍔W🍔or🍔ds 🍔in it🍔al🍔ic🍔s in the 🍔book\n",
      "🍔        🍔        🍔  🍔are 🍔enclos🍔ed by 🍔under🍔sco🍔re🍔s (🍔_) 🍔in this 🍔eBook\n",
      "\n",
      "🍔Project Gutenberg of Australia 🍔eBook🍔s are 🍔created 🍔from 🍔printed 🍔edition🍔s\n",
      "🍔which are 🍔in the 🍔public 🍔domain 🍔in 🍔Australi🍔a, 🍔unless 🍔a 🍔copy🍔right 🍔notic\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our tokenized text:\n",
    "debug(tokenized_text[:100], vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the [bible](data/bible.txt) (~4.3MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is of length: 4,351,186\n",
      "First 100 chars: '1:1 In the beginning God created the heaven and the earth.\\n\\n1:2 And the earth was without form, and '\n",
      "Training tokenizer on text of length 4,351,186 with vocab of size 100,000.\n",
      "build_indexed_list took 5.62 seconds.\n",
      "init_pairs_stats took 0.47 seconds.\n",
      "Training took 24.49 seconds.\n",
      "Tokenization took 14.17 seconds.\n",
      "Tokenized text has 454086 tokens.\n",
      "Detokenize took 0.15 seconds.\n"
     ]
    }
   ],
   "source": [
    "tokenized_text, vocab = train_and_test(r'data\\bible.txt', 100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's try a json file containing a large corpus of jokes from Reddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text is of length: 68,662,116\n",
      "First 100 chars: '[\\n    {\\n        \"body\": \"Now I have to say \\\\\"Leroy can you please paint the fence?\\\\\"\",\\n        \"id\":'\n",
      "Training tokenizer on text of length 68,662,116 with vocab of size 100,000.\n",
      "build_indexed_list took 76.74 seconds.\n",
      "init_pairs_stats took 7.86 seconds.\n",
      "Training took 293.86 seconds.\n",
      "Tokenization took 216.13 seconds.\n",
      "Tokenized text has 8064036 tokens.\n",
      "Detokenize took 2.82 seconds.\n"
     ]
    }
   ],
   "source": [
    "tokenized_text, vocab = train_and_test(r'data\\reddit_jokes.json', 100_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
